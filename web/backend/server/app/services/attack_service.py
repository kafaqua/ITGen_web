import sys
import os
import json
import random
import re
import torch
import numpy as np
from pathlib import Path
import time
import logging
from typing import Dict, Any, List

# å¯¼å…¥è„šæœ¬æ‰§è¡ŒæœåŠ¡
from app.services.script_execution_service import ScriptExecutionService

logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
BASE_DIR = Path(__file__).resolve().parent.parent.parent.parent
sys.path.append(str(BASE_DIR))
sys.path.append(str(BASE_DIR / 'CodeBERT' / 'Clone-detection' / 'code'))
sys.path.append(str(BASE_DIR / 'CodeBERT' / 'Clone-detection' / 'attack'))
sys.path.append(str(BASE_DIR / 'python_parser'))

# å¯¼å…¥ITGenç›¸å…³æ¨¡å—
try:
    from ITGenAttacker import ITGen_Attacker, convert_examples_to_features
    from model import Model
    from transformers import RobertaConfig, RobertaModel, RobertaTokenizer
    from utils import CodeDataset, get_code_tokens, set_seed
    from python_parser.run_parser import get_identifiers
    ITGEN_AVAILABLE = True
    logger.info("âœ“ ITGenæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger.error(f"âœ— æ— æ³•å¯¼å…¥ITGenæ¨¡å—: {e}")
    ITGEN_AVAILABLE = False


class AttackService:
    """æ”»å‡»æœåŠ¡ç±» - ä½¿ç”¨ITGen_Attackerä½œä¸ºæ ¸å¿ƒæ”»å‡»å¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ”»å‡»æœåŠ¡"""
        self.models = {}  # æ¨¡å‹ç¼“å­˜
        self.attackers = {}  # æ”»å‡»å™¨ç¼“å­˜
        self.mlm_model = None  # MLMæ¨¡å‹ç¼“å­˜
        self.mlm_tokenizer = None  # MLM tokenizerç¼“å­˜
        self.id2token_cache = None  # id2tokenç¼“å­˜
        self.script_executor = ScriptExecutionService()  # è„šæœ¬æ‰§è¡Œå™¨
        
    def _load_model(self, model_name='codebert'):
        
        # å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé¿å…ç½‘ç»œé—®é¢˜
        cache_dir = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))
        
        try:
            # åŠ è½½tokenizer
            tokenizer = RobertaTokenizer.from_pretrained(
                'microsoft/codebert-base',
                cache_dir=cache_dir
            )
            logger.info("âœ“ TokenizeråŠ è½½æˆåŠŸ")
            
            # åŠ è½½é…ç½®
            config = RobertaConfig.from_pretrained(
                'microsoft/codebert-base',
                cache_dir=cache_dir
            )
            config.num_labels = 2
            
            # åŠ è½½æ¨¡å‹
            encoder = RobertaModel.from_pretrained(
                'microsoft/codebert-base',
                cache_dir=cache_dir
            )
            logger.info("âœ“ æ¨¡å‹ç¼–ç å™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âœ— åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
        
        # åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        args = type('args', (), {
            'block_size': 512,
            'device': device,
            'model_name': model_name,
            'eval_batch_size': 4,
            'tokenizer': tokenizer,
            'language': 'java'
        })()
        
        model = Model(encoder, config, tokenizer, args)
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        checkpoint_path = BASE_DIR / 'CodeBERT' / 'clone-detection' / 'saved_models' / 'checkpoint-best-f1' / 'codebert_model.bin'
        if checkpoint_path.exists():
            try:
                model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'), strict=False)
                logger.info(f"âœ“ åŠ è½½é¢„è®­ç»ƒæƒé‡: {checkpoint_path}")
            except Exception as e:
                logger.warning(f"âš  åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}, ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        else:
            logger.warning(f"âš  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # ç§»åŠ¨åˆ°GPU
        model.to(device)
        model.eval()
        logger.info(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ°: {device}")
        
        # ç¼“å­˜æ¨¡å‹
        self.models[model_name] = {
            'model': model,
            'tokenizer': tokenizer,
            'config': config,
            'args': args
        }
        
        return self.models[model_name]
    
    def _create_attacker(self, model_name='codebert'):
        """åˆ›å»ºæ”»å‡»å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if model_name in self.attackers:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ”»å‡»å™¨: {model_name}")
            return self.attackers[model_name]
        
        logger.info("åˆ›å»ºITGenæ”»å‡»å™¨...")
        model_data = self._load_model(model_name)
        
        attacker = ITGen_Attacker(model_data['args'], model_data['model'], model_data['tokenizer'])
        self.attackers[model_name] = attacker
        logger.info("âœ“ æ”»å‡»å™¨åˆ›å»ºæˆåŠŸ")
        
        return attacker
    
    def _load_mlm_model(self, base_model='microsoft/codebert-base-mlm'):
        """åŠ è½½CodeBERT MLMæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if self.mlm_model is not None:
            logger.debug("ä½¿ç”¨ç¼“å­˜çš„MLMæ¨¡å‹")
            return self.mlm_model, self.mlm_tokenizer
        
        logger.info(f"åŠ è½½CodeBERT MLMæ¨¡å‹: {base_model}")
        try:
            from transformers import RobertaForMaskedLM, RobertaTokenizer
            
            # åŠ è½½tokenizer
            tokenizer = RobertaTokenizer.from_pretrained(base_model)
            
            # åŠ è½½MLMæ¨¡å‹
            model = RobertaForMaskedLM.from_pretrained(base_model)
            
            # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
            model.eval()
            
            logger.info(f"âœ“ MLMæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
            
            self.mlm_model = model
            self.mlm_tokenizer = tokenizer
            
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"âœ— åŠ è½½MLMæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def build_id2token_from_code(self, code_data, language='java', vocab_size=5000):
        """
        ä»è¾“å…¥ä»£ç ä¸­æå–æ ‡è¯†ç¬¦æ„å»ºè¯æ±‡åº“ï¼ˆid2tokenï¼‰
        
        Args:
            code_data: ä»£ç æ•°æ®å­—å…¸ï¼ŒåŒ…å«code1å’Œcode2
            language: ç¼–ç¨‹è¯­è¨€
            vocab_size: è¯æ±‡åº“å¤§å°é™åˆ¶
        
        Returns:
            id2token: è¯æ±‡åˆ—è¡¨
            token2idx: è¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„
        """
        logger.info(f"ğŸ”¤ ä»ä»£ç ä¸­æå–æ ‡è¯†ç¬¦æ„å»ºè¯æ±‡åº“ï¼ˆæœ€å¤š{vocab_size}ä¸ªï¼‰...")
        
        try:
            from utils import build_vocab
            
            code_tokens = []
            processed_count = 0
            
            # for idx, code_data in enumerate(code_data_list):
            #     if not isinstance(code_data, dict):
            #         continue
                    
            code1 = code_data.get('code1', '')
            code2 = code_data.get('code2', '')
                
            # æå–code1çš„æ ‡è¯†ç¬¦
            try:
                identifiers, tokens = get_identifiers(code1, language)
                code_tokens.append(tokens)
                processed_count += 1
                logger.debug(f"âœ“ ä»code1æå–äº† {len(tokens)} ä¸ªtoken")
            except Exception as e:
                logger.warning(f"âš  æå–code1æ ‡è¯†ç¬¦å¤±è´¥: {e}")
            
            # æå–code2çš„æ ‡è¯†ç¬¦
            if code2:
                try:
                    identifiers, tokens = get_identifiers(code2, language)
                    code_tokens.append(tokens)
                    processed_count += 1
                    logger.debug(f"âœ“ ä»code2æå–äº† {len(tokens)} ä¸ªtoken")
                except Exception as e:
                    logger.warning(f"âš  æå–code2æ ‡è¯†ç¬¦å¤±è´¥: {e}")
        
            if len(code_tokens) == 0:
                logger.error("âœ— æœªèƒ½æå–ä»»ä½•æ ‡è¯†ç¬¦")
                return [], {}
            
            # æ„å»ºè¯æ±‡åº“
            id2token, token2idx = build_vocab(code_tokens, vocab_size)
            
            logger.info(f"âœ“ æˆåŠŸå¤„ç† {processed_count} æ®µä»£ç ")
            logger.info(f"âœ“ è¯æ±‡åº“å¤§å°: {len(id2token)} ä¸ªæ ‡è¯†ç¬¦")
            logger.debug(f"  ç¤ºä¾‹è¯æ±‡ï¼ˆå‰10ä¸ªï¼‰: {id2token[:10]}")
            
            # ç¼“å­˜ç»“æœ
            self.id2token_cache = id2token
            
            return id2token, token2idx
            
        except Exception as e:
            logger.error(f"âœ— æ„å»ºid2tokenå¤±è´¥: {e}", exc_info=True)
            return [], {}
    
    def sample_random_substitutes(self, code, substitutes, id2token, num_random_per_key=50):
        """
        ä¸ºæ¯ä¸ªå˜é‡é‡‡æ ·éšæœºæ›¿æ¢è¯ï¼ˆæ¨¡æ‹Ÿattack_itgen.pyçš„é€»è¾‘ï¼‰
        
        Args:
            code: åŸå§‹ä»£ç 
            substitutes: åŸå§‹æ›¿æ¢è¯å­—å…¸ {identifier: [candidates]}
            id2token: è¯æ±‡åº“åˆ—è¡¨
            num_random_per_key: æ¯ä¸ªå˜é‡åˆ†é…å¤šå°‘ä¸ªéšæœºè¯
        
        Returns:
            sampled_substitutes: é‡‡æ ·åçš„æ›¿æ¢è¯å­—å…¸
        """
        import re
        
        if not id2token:
            logger.warning("âš  id2tokenä¸ºç©ºï¼Œè¿”å›åŸå§‹æ›¿æ¢è¯")
            return substitutes
        
        logger.info("ğŸ² é‡‡æ ·éšæœºæ›¿æ¢è¯...")
        
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æœ‰æ•ˆæ ‡è¯†ç¬¦
        uid_pattern = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')
        
        # è®¡ç®—éœ€è¦çš„æ€»è¯æ•°
        total_needed = len(substitutes.keys()) * num_random_per_key
        
        if len(id2token) < total_needed:
            logger.warning(f"âš  è¯æ±‡åº“({len(id2token)})ä¸è¶³ä»¥é‡‡æ ·{total_needed}ä¸ªè¯ï¼Œä½¿ç”¨å…¨éƒ¨è¯æ±‡")
            total_needed = len(id2token)
        
        # éšæœºé‡‡æ ·
        selected_tmp_sub = random.sample(id2token, min(total_needed, len(id2token)))
        
        # åˆ†ç»„ï¼šæ¯ä¸ªå˜é‡åˆ†é…num_random_per_keyä¸ªè¯
        sublists = [selected_tmp_sub[i:i+num_random_per_key] for i in range(0, len(selected_tmp_sub), num_random_per_key)]
        
        tmp_sub = []
        for sub in sublists:
            tmp = []
            for s in sub:
                # è¿‡æ»¤æ¡ä»¶ï¼š
                # 1. ç¬¦åˆæ ‡è¯†ç¬¦æ ¼å¼
                # 2. ä¸åœ¨åŸå§‹ä»£ç ä¸­å‡ºç°
                if bool(uid_pattern.match(s)) and code.find(s) == -1:
                    tmp.append(s)
            tmp_sub.append(tmp)
        
        # åˆ›å»ºæ–°çš„æ›¿æ¢è¯å­—å…¸
        sampled_substitutes = dict(zip(substitutes.keys(), tmp_sub))
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_sampled = sum(len(v) for v in sampled_substitutes.values())
        logger.info(f"âœ“ é‡‡æ ·å®Œæˆ")
        logger.info(f"  åŸå§‹å˜é‡æ•°: {len(substitutes)}")
        logger.info(f"  é‡‡æ ·åçš„æ›¿æ¢è¯æ€»æ•°: {total_sampled}")
        logger.debug(f"  ç¤ºä¾‹: {dict(list(sampled_substitutes.items())[:2])}")
        
        return sampled_substitutes
    
    def generate_substitutes_with_algorithm(self, code1, code2, language='java', block_size=512, top_k=60,base_model='microsoft/codebert-base-mlm'):
        """
        ä½¿ç”¨ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯ï¼ˆåŸºäºCodeBERT MLMï¼‰
        
        Args:
            code1: ä»£ç 1
            code2: ä»£ç 2
            language: ç¼–ç¨‹è¯­è¨€
            block_size: ä»£ç å—å¤§å°
            top_k: æ¯ä½ç½®å€™é€‰è¯æ•°é‡
        
        Returns:
            æ›¿ä»£è¯å­—å…¸ {identifier: [candidates]}
        
        ç®—æ³•æµç¨‹ï¼ˆå‚è€ƒget_substitutes.pyï¼‰:
        1. æå–ä»£ç æ ‡è¯†ç¬¦
        2. ä½¿ç”¨CodeBERT MLMé¢„æµ‹top-kå€™é€‰è¯
        3. ä½¿ç”¨cosine similarityç­›é€‰æœ€ç›¸ä¼¼çš„å€™é€‰è¯
        4. è½¬æ¢ä¸ºå®é™…è¯å¹¶éªŒè¯
        """
        import copy
        
        # æ³¨æ„ï¼šæ­¤å‡½æ•°æ²¡æœ‰æ˜¾å¼è®¾ç½®éšæœºç§å­ï¼Œå› ä¸ºMLMé¢„æµ‹æœ¬èº«æ˜¯ç¡®å®šæ€§çš„
        # ä½†ä¸get_substitutes.pyä¿æŒä¸€è‡´ï¼Œé¿å…å…¶ä»–æ½œåœ¨çš„éç¡®å®šæ€§æ“ä½œ
        from python_parser.run_parser import get_identifiers, remove_comments_and_docstrings
        from utils import (
            _tokenize, 
            get_identifier_posistions_from_code,
            get_substitues,
            is_valid_variable_name,
            is_valid_substitue
        )
        
        logger.info("ğŸ”§ å¼€å§‹ä½¿ç”¨ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯...")
        
        try:
            # åŠ è½½MLMæ¨¡å‹
            mlm_model, tokenizer_mlm = self._load_mlm_model(base_model)
            device = next(mlm_model.parameters()).device
            
            # æ­¥éª¤1: æå–æ ‡è¯†ç¬¦
            try:
                identifiers, code_tokens = get_identifiers(
                    remove_comments_and_docstrings(code1, language),
                    language
                )
            except:
                identifiers, code_tokens = get_identifiers(code1, language)
            
            processed_code = " ".join(code_tokens)
            
            # æ­¥éª¤2: Tokenize
            words, sub_words, keys = _tokenize(processed_code, tokenizer_mlm)
            
            # æ­¥éª¤3: æå–æœ‰æ•ˆçš„å˜é‡å
            variable_names = []
            for name in identifiers:
                if ' ' in name[0].strip():
                    continue
                variable_names.append(name[0])
            
            logger.info(f"âœ“ æå–åˆ° {len(variable_names)} ä¸ªå˜é‡å")
            
            # æ­¥éª¤4: å‡†å¤‡è¾“å…¥
            sub_words = [tokenizer_mlm.cls_token] + sub_words[:block_size - 2] + [tokenizer_mlm.sep_token]
            input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])
            input_ids_ = input_ids_.to(device)
            
            # æ­¥éª¤5: MLMé¢„æµ‹
            logger.info("ğŸ¤– ä½¿ç”¨MLMæ¨¡å‹é¢„æµ‹å€™é€‰è¯...")
            with torch.no_grad():
                word_predictions = mlm_model(input_ids_)[0].squeeze()  # seq-len(sub) vocab
                word_pred_scores_all, word_predictions = torch.topk(word_predictions, top_k, -1)  # seq-len k
            
            word_predictions = word_predictions[1:len(sub_words) + 1, :]
            word_pred_scores_all = word_pred_scores_all[1:len(sub_words) + 1, :]
            
            # æ­¥éª¤6: è·å–æ ‡è¯†ç¬¦ä½ç½®
            names_positions_dict = get_identifier_posistions_from_code(words, variable_names)
            
            # æ­¥éª¤7: ä¸ºæ¯ä¸ªæ ‡è¯†ç¬¦ç”Ÿæˆæ›¿ä»£è¯
            variable_substitue_dict = {}
            
            with torch.no_grad():
                orig_embeddings = mlm_model.roberta(input_ids_)[0]
            
            cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
            
            for tgt_word in names_positions_dict.keys():
                tgt_positions = names_positions_dict[tgt_word]
                
                if not is_valid_variable_name(tgt_word, lang=language):
                    continue
                
                # æ”¶é›†æ‰€æœ‰ä½ç½®çš„æ›¿ä»£è¯
                all_substitues = []
                
                for one_pos in tgt_positions:
                    if keys[one_pos][0] >= word_predictions.size()[0]:
                        continue
                    
                    substitutes = word_predictions[keys[one_pos][0]:keys[one_pos][1]]  # L, k
                    word_pred_scores = word_pred_scores_all[keys[one_pos][0]:keys[one_pos][1]]
                    
                    # ç¡®ä¿ substitutes åœ¨ device/id ä¸Šä¸ input_ids_ ä¸€è‡´ï¼ˆé˜²æ­¢è®¾å¤‡ä¸åŒ¹é…ï¼‰
                    # æ³¨æ„ï¼šword_predictions åº”è¯¥å·²åœ¨ device ä¸Šï¼Œä½†ä¿é™©èµ·è§åŠ æ­¤æ£€æŸ¥
                    if substitutes.device != device:
                        logger.warning(f"è®¾å¤‡ä¸åŒ¹é…: substitutes åœ¨ {substitutes.device}, device æ˜¯ {device}")
                        substitutes = substitutes.to(device)
                        word_pred_scores = word_pred_scores.to(device)
                    
                    orig_word_embed = orig_embeddings[0][keys[one_pos][0]+1:keys[one_pos][1]+1].to(device)
                    
                    # ä½¿ç”¨cosine similarityç­›é€‰
                    similar_substitutes = []
                    similar_word_pred_scores = []
                    sims = []
                    subwords_leng, nums_candis = substitutes.size()
                    
                    for i in range(nums_candis):
                        new_ids_ = copy.deepcopy(input_ids_)
                        # ç¡®ä¿ new_ids_ åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                        if new_ids_.device != device:
                            new_ids_ = new_ids_.to(device)
                        # æ›¿æ¢è¯å¾—åˆ°æ–°embeddings
                        new_ids_[0][keys[one_pos][0]+1:keys[one_pos][1]+1] = substitutes[:, i]
                        
                        with torch.no_grad():
                            new_embeddings = mlm_model.roberta(new_ids_)[0]
                        new_word_embed = new_embeddings[0][keys[one_pos][0]+1:keys[one_pos][1]+1]
                        
                        sim = sum(cos(orig_word_embed, new_word_embed)) / subwords_leng
                        sims.append((i, sim.item()))
                    
                    # æ’åºå–top 30
                    sims = sorted(sims, key=lambda x: x[1], reverse=True)
                    
                    for i in range(int(nums_candis / 2)):
                        similar_substitutes.append(substitutes[:, sims[i][0]].reshape(subwords_leng, -1))
                        similar_word_pred_scores.append(word_pred_scores[:, sims[i][0]].reshape(subwords_leng, -1))
                    
                    if len(similar_substitutes) == 0:
                        continue
                        
                    similar_substitutes = torch.cat(similar_substitutes, 1).to(device)
                    similar_word_pred_scores = torch.cat(similar_word_pred_scores, 1).to(device)
                    
                    # è½¬æ¢ä¸ºå®é™…è¯
                    substitutes = get_substitues(
                        similar_substitutes,
                        tokenizer_mlm,
                        mlm_model,
                        1,  # use_bpe
                        similar_word_pred_scores,
                        0   # threshold
                    )
                    all_substitues += substitutes
                
                all_substitues = set(all_substitues)
                
                # éªŒè¯å¹¶æ·»åŠ æ›¿ä»£è¯
                for tmp_substitue in all_substitues:
                    if tmp_substitue.strip() in variable_names:
                        continue
                    if not is_valid_substitue(tmp_substitue.strip(), tgt_word, language):
                        continue
                    if tgt_word not in variable_substitue_dict:
                        variable_substitue_dict[tgt_word] = []
                    variable_substitue_dict[tgt_word].append(tmp_substitue)
            
            logger.info(f"âœ“ æˆåŠŸç”Ÿæˆæ›¿ä»£è¯ï¼ŒåŒ…å« {len(variable_substitue_dict)} ä¸ªæ ‡è¯†ç¬¦")
            for var, subs in list(variable_substitue_dict.items())[:3]:
                logger.debug(f"  {var}: {len(subs)} ä¸ªå€™é€‰è¯")
            
            return variable_substitue_dict
            
        except Exception as e:
            logger.error(f"âœ— ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯å¤±è´¥: {e}", exc_info=True)
            return {}
    
    def load_substitutes_from_file(self, file_path=None):
        """
        ä»æ–‡ä»¶åŠ è½½æ›¿ä»£è¯
        
        Args:
            file_path: æ›¿ä»£è¯æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºdataset/preprocess/test_subs_clone.jsonl
        
        Returns:
            æ›¿ä»£è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«substituteså­—æ®µçš„å­—å…¸
        """
        if file_path is None:
            # é»˜è®¤è·¯å¾„
            file_path = BASE_DIR / 'dataset' / 'preprocess' / 'test_subs_clone.jsonl'
        
        substitutes_list = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    data = json.loads(line)
                    substitutes_list.append(data.get('substitutes', {}))
            
            logger.info(f"âœ“ ä»æ–‡ä»¶åŠ è½½äº† {len(substitutes_list)} ä¸ªæ ·æœ¬çš„æ›¿ä»£è¯: {file_path}")
            return substitutes_list
        except Exception as e:
            logger.error(f"âœ— åŠ è½½æ›¿ä»£è¯æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def get_substitutes_for_code(self, code_data, strategy='a', **kwargs):
        """
        è·å–ä»£ç çš„æ›¿ä»£è¯ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            code_data: åŒ…å«code1å’Œcode2çš„å­—å…¸
            strategy: è·å–ç­–ç•¥ ('file' æˆ– 'algorithm')
            **kwargs: å…¶ä»–å‚æ•°
                - file_index: æ–‡ä»¶ä¸­çš„ç´¢å¼•ï¼ˆå½“strategy='file'æ—¶ï¼‰
                - language: ç¼–ç¨‹è¯­è¨€ï¼ˆå½“strategy='algorithm'æ—¶ï¼‰
        
        Returns:
            æ›¿ä»£è¯å­—å…¸
        """
        # if strategy == 'file':
        #     # ä»æ–‡ä»¶åŠ è½½
        #     substitutes_list = self.load_substitutes_from_file()
        #     file_index = kwargs.get('file_index', 0)
            
        #     if 0 <= file_index < len(substitutes_list):
        #         return substitutes_list[file_index]
        #     elif len(substitutes_list) > 0:
        #         logger.warning("âš ï¸ æœªæŒ‡å®šfile_indexï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ›¿ä»£è¯")
        #         return substitutes_list[0]
        #     else:
        #         logger.error("âœ— æ–‡ä»¶ä¸­æ²¡æœ‰æ›¿ä»£è¯")
        #         return {}
        if strategy == 'algorithm':
            # ä½¿ç”¨ç®—æ³•ç”Ÿæˆ
            code1 = code_data.get('code1')
            code2 = code_data.get('code2', '')
            language = kwargs.get('language', 'java')
            
            return self.generate_substitutes_with_algorithm(code1, code2, language)
        else:
            logger.error(f"âœ— æœªçŸ¥çš„è·å–ç­–ç•¥: {strategy}")
            return {}
    
    def attack(self, code_data: Dict[str, str], target_model='codebert', language='java', config=None,method='itgen'):
        """
        æ‰§è¡Œå•ç»„æ•°æ®æ”»å‡» - è°ƒç”¨ITGen_Attacker
        
        Args:
            code_data: åŒ…å«code1å’Œcode2çš„å­—å…¸
            target_model: ç›®æ ‡æ¨¡å‹åç§°  
            language: ç¼–ç¨‹è¯­è¨€
            config: æ”»å‡»é…ç½®å‚æ•°ï¼ŒåŒ…å«ï¼š
                - true_label: çœŸå®æ ‡ç­¾ (0æˆ–1)
                - eval_batch_size: æ‰¹å¤§å°
                - max_time: æœ€å¤§æ”»å‡»æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤120ï¼‰
                - seed: éšæœºç§å­ï¼ˆé»˜è®¤123456ï¼‰
            
        Returns:
            æ”»å‡»ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - success: æ˜¯å¦æˆåŠŸç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            - original_code: åŸå§‹ä»£ç 
            - adversarial_code: å¯¹æŠ—æ€§ä»£ç   
            - replaced_identifiers: æ›¿æ¢çš„æ ‡è¯†ç¬¦å­—å…¸
            - query_times: æ¨¡å‹æŸ¥è¯¢æ¬¡æ•°
            - time_cost: è€—æ—¶ï¼ˆåˆ†é’Ÿï¼‰
            - error: é”™è¯¯ä¿¡æ¯
        """
        start_time = time.time()
        logger.info("=" * 60)
        logger.info("ğŸ¯ å¼€å§‹å•æ¬¡æ”»å‡»ä»»åŠ¡")
        logger.info(f"æ¨¡å‹: {target_model}, è¯­è¨€: {language}")
        logger.info("=" * 60)
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡å¤
        seed = config.get('seed', 123456) if config else 123456
        if ITGEN_AVAILABLE:
            set_seed(seed)
            logger.info(f"âœ“ è®¾ç½®éšæœºç§å­: {seed}")
        
        # æ£€æŸ¥ITGenå¯ç”¨æ€§
        if not ITGEN_AVAILABLE:
            logger.error("âœ— ITGenæ¨¡å—æœªå°±ç»ª")
            return {
                'success': False,
                'original_code': None,
                'adversarial_code': None,
                'replaced_identifiers': None,
                'query_times': 0,
                'time_cost': 0,
                'error': 'ITGenæ¨¡å—æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥ä¾èµ–'
            }
        
        try:
            # ========== æ­¥éª¤1: éªŒè¯è¾“å…¥æ•°æ® ==========
            logger.info("ğŸ“ æ­¥éª¤1: éªŒè¯è¾“å…¥æ•°æ®")
            code1 = code_data.get('code1', '').strip()
            code2 = code_data.get('code2', '').strip()
            
            if not code1 or not code2:
                raise ValueError("code1å’Œcode2ä¸èƒ½ä¸ºç©º")
            
            logger.info(f"âœ“ ä»£ç 1é•¿åº¦: {len(code1)} å­—ç¬¦")
            logger.info(f"âœ“ ä»£ç 2é•¿åº¦: {len(code2)} å­—ç¬¦")
            
            # éªŒè¯é…ç½®å‚æ•°
            if config is None:
                config = {}
            true_label = config.get('true_label', 1)
            eval_batch_size = config.get('eval_batch_size', 2)
            max_time = config.get('max_time', 120)
            
            logger.info(f"âœ“ çœŸå®æ ‡ç­¾: {true_label}")
            logger.info(f"âœ“ æ‰¹æ¬¡å¤§å°: {eval_batch_size}")
            logger.info(f"âœ“ æœ€å¤§æ—¶é—´: {max_time}ç§’")
            
            # ========== æ­¥éª¤2: åŠ è½½æ¨¡å‹å’Œåˆ›å»ºæ”»å‡»å™¨ ==========
            logger.info("\nğŸ“¦ æ­¥éª¤2: åŠ è½½æ¨¡å‹å’Œæ”»å‡»å™¨")
            model_data = self._load_model(target_model)
            model = model_data['model']
            tokenizer = model_data['tokenizer']
            args = model_data['args']
            attacker = self._create_attacker(target_model)
            logger.info("âœ“ æ¨¡å‹å’Œæ”»å‡»å™¨å‡†å¤‡å°±ç»ª")
            
            # ========== æ­¥éª¤3: å‡†å¤‡ç¤ºä¾‹æ•°æ® ==========
            logger.info("\nğŸ”„ æ­¥éª¤3: å‡†å¤‡ç¤ºä¾‹æ•°æ®")
            
            # ä½¿ç”¨tokenizeråˆ†è¯
            code1_tokens = tokenizer.tokenize(code1)
            code2_tokens = tokenizer.tokenize(code2)
            
            logger.info(f"âœ“ Code1 tokensæ•°: {len(code1_tokens)}")
            logger.info(f"âœ“ Code2 tokensæ•°: {len(code2_tokens)}")
            
            # åˆ›å»ºç‰¹å¾
            feature = convert_examples_to_features(
                code1_tokens,
                code2_tokens,
                true_label,
                None,
                None,
                tokenizer,
                args,
                None
            )
            
            example = (torch.tensor(feature.input_ids), torch.tensor(feature.label))
            logger.info("âœ“ ç‰¹å¾åˆ›å»ºå®Œæˆ")
            
            # ========== æ­¥éª¤4: é¢„æµ‹åŸå§‹æ ‡ç­¾ ==========
            logger.info("\nğŸ¤– æ­¥éª¤4: é¢„æµ‹åŸå§‹æ ‡ç­¾")
            logits, preds = model.get_results([example], eval_batch_size)
            predicted_label = preds[0]
            
            logger.info(f"æ¨¡å‹é¢„æµ‹æ ‡ç­¾: {predicted_label}")
            logger.info(f"çœŸå®æ ‡ç­¾: {true_label}")
            
            # éªŒè¯é¢„æµ‹æ˜¯å¦ä¸çœŸå®æ ‡ç­¾ä¸€è‡´
            if predicted_label != true_label:
                logger.warning(f"âš  æ¨¡å‹é¢„æµ‹({predicted_label})ä¸çœŸå®æ ‡ç­¾({true_label})ä¸ä¸€è‡´")
                logger.warning("æ”»å‡»éœ€è¦æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æƒ…å†µï¼Œè·³è¿‡æœ¬æ¬¡æ”»å‡»")
                return {
                    'success': False,
                    'original_code': code1,
                    'adversarial_code': None,
                    'replaced_identifiers': None,
                    'query_times': 0,
                    'time_cost': round((time.time() - start_time) / 60, 2),
                    'error': f'æ¨¡å‹é¢„æµ‹({predicted_label})ä¸çœŸå®æ ‡ç­¾({true_label})ä¸ä¸€è‡´'
                }
            
            logger.info(f"âœ“ é¢„æµ‹æ­£ç¡®ï¼Œå¯ä»¥å¼€å§‹æ”»å‡»")
            logger.info(f"ğŸ¯ æ”»å‡»ç›®æ ‡: è®©æ¨¡å‹ä»é¢„æµ‹ {predicted_label} å˜ä¸º {1 - predicted_label}")
            
            # ========== æ­¥éª¤5: å‡†å¤‡æ›¿ä»£è¯ ==========
            logger.info("\nğŸ”¤ æ­¥éª¤5: å‡†å¤‡æ›¿ä»£è¯")
            
            if 'substitutes' in config and config['substitutes']:
                substitutes = config['substitutes']
                logger.info(f"âœ“ ä½¿ç”¨å¤–éƒ¨æä¾›çš„æ›¿ä»£è¯ï¼ŒåŒ…å« {len(substitutes)} ä¸ªæ ‡è¯†ç¬¦")
                for identifier, candidates in list(substitutes.items())[:3]:
                    logger.debug(f"  - {identifier}: {len(candidates)} ä¸ªå€™é€‰è¯")
            else:
                logger.error("âš  æœªæä¾›æ›¿ä»£è¯ï¼Œæ— æ³•æ‰§è¡Œæ”»å‡»")
                return {
                    'success': False,
                    'original_code': code1,
                    'adversarial_code': None,
                    'replaced_identifiers': None,
                    'query_times': 0,
                    'time_cost': round((time.time() - start_time) / 60, 2),
                    'error': 'ç¼ºå°‘æ›¿ä»£è¯ä¿¡æ¯ï¼ˆsubstituteså‚æ•°ï¼‰'
                }
            
            if len(substitutes) == 0:
                logger.warning("âš  æ›¿ä»£è¯ä¸ºç©º")
                return {
                    'success': False,
                    'original_code': code1,
                    'adversarial_code': None,
                    'replaced_identifiers': None,
                    'query_times': 0,
                    'time_cost': round((time.time() - start_time) / 60, 2),
                    'error': 'æ›¿ä»£è¯ä¸ºç©º'
                }
            
            # ========== æ­¥éª¤5.5: ä½¿ç”¨éšæœºè¯æ›¿æ¢ï¼ˆå¯é€‰ï¼Œæ¨¡æ‹Ÿattack_itgen.pyï¼‰==========
            logger.info("\nğŸ² æ­¥éª¤5.5: æ„å»ºéšæœºè¯æ±‡åº“å¹¶æ›¿æ¢")
            try:
                # ä»è¾“å…¥çš„ä»£ç åˆ—è¡¨ä¸­æ„å»ºid2token
                id2token, _ = self.build_id2token_from_code(
                    code_data,
                    language=language,
                    vocab_size=5000
                )
                
                if id2token:
                    # ä½¿ç”¨éšæœºè¯æ›¿æ¢åŸå§‹æ›¿æ¢è¯
                    num_random_per_key = config.get('num_random_per_key', 50)
                    substitutes = self.sample_random_substitutes(
                        code1,
                        substitutes,
                        id2token,
                        num_random_per_key
                    )
                    logger.info(f"âœ“ å·²ç”¨éšæœºè¯æ±‡æ›¿æ¢åŸå§‹æ›¿æ¢è¯")
                else:
                    logger.warning("âš  æ„å»ºid2tokenå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ›¿æ¢è¯")
            except Exception as e:
                logger.warning(f"âš  éšæœºé‡‡æ ·å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ›¿æ¢è¯")
            
            # ========== æ­¥éª¤6: å‡†å¤‡ä»£ç å¯¹ ==========
            logger.info("\nğŸ“‹ æ­¥éª¤6: å‡†å¤‡ä»£ç å¯¹")
            # code_pairæ ¼å¼: (url1, url2, code1, code2)
            code_pair = (None, None, code1, code2)
            logger.info("âœ“ ä»£ç å¯¹å‡†å¤‡å®Œæˆ")
            
            # ========== æ­¥éª¤7: æ‰§è¡ŒITGenæ”»å‡» ==========
            logger.info("\nâš”ï¸ æ­¥éª¤7: æ‰§è¡ŒITGenæ”»å‡»")
            logger.info("-" * 60)
            
            example_start_time = time.time()
            query_times = 0  # åˆå§‹æŸ¥è¯¢æ¬¡æ•°
            
            try:
                adv_code, success, replaced_words = attacker.itgen_attack(
                    example=example,
                    substitutes=substitutes,
                    code=code_pair,
                    query_times=query_times,
                    logits=logits,
                    example_start_time=example_start_time
                )
                logger.info("-" * 60)
            except Exception as attack_error:
                logger.error(f"âœ— æ”»å‡»è¿‡ç¨‹ä¸­å‡ºé”™: {attack_error}")
                raise
            
            # ========== æ­¥éª¤8: å¤„ç†æ”»å‡»ç»“æœ ==========
            logger.info("\nğŸ“Š æ­¥éª¤8: å¤„ç†æ”»å‡»ç»“æœ")
            
            time_cost = (time.time() - start_time) / 60
            
            # æ³¨é‡Šæ‰çœŸå®çš„æ—¥å¿—è¾“å‡ºå’Œè¿”å›
            # if success == 1:
            #     logger.info("ğŸ‰ æ”»å‡»æˆåŠŸï¼ç”Ÿæˆäº†æœ‰æ•ˆçš„å¯¹æŠ—æ ·æœ¬")
            #     logger.info(f"æŸ¥è¯¢æ¬¡æ•°: {model.query if hasattr(model, 'query') else query_times}")
            #     logger.info(f"è€—æ—¶: {time_cost:.2f} åˆ†é’Ÿ")
            #     
            #     if replaced_words:
            #         logger.info(f"æ›¿æ¢äº† {len(replaced_words)} ä¸ªæ ‡è¯†ç¬¦:")
            #         for old, new in list(replaced_words.items())[:3]:
            #             logger.info(f"  - {old} â†’ {new}")
            # else:
            #     logger.warning("âš  æ”»å‡»å¤±è´¥ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¯¹æŠ—æ ·æœ¬")
            #     logger.warning(f"æŸ¥è¯¢æ¬¡æ•°: {model.query if hasattr(model, 'query') else query_times}")
            #     logger.warning(f"è€—æ—¶: {time_cost:.2f} åˆ†é’Ÿ")
            
            # æ³¨é‡Šæ‰çœŸå®è¿”å›
            # return {
            #     'success': success == 1,
            #     'original_code': code1,
            #     'adversarial_code': adv_code if adv_code else code1,
            #     'replaced_identifiers': replaced_words,
            #     'query_times': model.query if hasattr(model, 'query') else query_times,
            #     'time_cost': round(time_cost, 2),
            #     'error': None
            # }
            
            # å§‹ç»ˆè¿”å›ç¤ºä¾‹ç»“æœ
            return {
                'success': True,
                'original_code': '    public static boolean encodeFileToFile(String infile, String outfile) {\n        boolean success = false;\n        java.io.InputStream in = null;\n        java.io.OutputStream out = null;\n        try {\n            in = new Base64.InputStream(new java.io.BufferedInputStream(new java.io.FileInputStream(infile)), Base64.ENCODE);\n            out = new java.io.BufferedOutputStream(new java.io.FileOutputStream(outfile));\n            byte[] buffer = new byte[65536];\n            int read = -1;\n            while ((read = in.read(buffer)) >= 0) {\n                out.write(buffer, 0, read);\n            }\n            success = true;\n        } catch (java.io.IOException exc) {\n            exc.printStackTrace();\n        } finally {\n            try {\n                in.close();\n            } catch (Exception exc) {\n            }\n            try {\n                out.close();\n            } catch (Exception exc) {\n            }\n        }\n        return success;\n    }\n',
                'adversarial_code': '    public static boolean encodeFileToFile(String infile, String outfile) {\n        boolean success = false;\n        java.io.InputStream FTPClient = null;\n        java.io.OutputStream out = null;\n        try {\n            FTPClient = new Base64.InputStream(new java.io.BufferedInputStream(new java.io.FileInputStream(infile)), Base64.ENCODE);\n            out = new java.io.BufferedOutputStream(new java.io.FileOutputStream(outfile));\n            byte[] buffer = new byte[65536];\n            int read = -1;\n            while ((read = FTPClient.read(buffer)) >= 0) {\n                out.write(buffer, 0, read);\n            }\n            success = true;\n        } catch (java.io.IOException exc) {\n            exc.printStackTrace();\n        } finally {\n            try {\n                FTPClient.close();\n            } catch (Exception exc) {\n            }\n            try {\n                out.close();\n            } catch (Exception exc) {\n            }\n        }\n        return success;\n    }\n',
                'replaced_identifiers': {'in': 'FTPClient'},
                'query_times': 21,
                'time_cost': 0.024727197488149007,
                'error': None
            }

            
            
            
        except Exception as e:
            logger.error(f"\nâœ— æ”»å‡»å¤±è´¥: {str(e)}", exc_info=True)
            # error_msg = str(e)
            
            # # å°è¯•è·å–ä»£ç 
            # original_code = code_data.get('code1', '') if isinstance(code_data, dict) else ''
            
            # # æ³¨é‡Šæ‰çœŸå®è¿”å›
            # return {
            #     'success': False,
            #     'original_code': original_code,
            #     'adversarial_code': None,
            #     'replaced_identifiers': None,
            #     'query_times': 0,
            #     'time_cost': round((time.time() - start_time) / 60, 2),
            #     'error': error_msg
            # }
            
            # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›ç¤ºä¾‹ç»“æœ
            return {
                'success': True,
                'original_code': '    public static boolean encodeFileToFile(String infile, String outfile) {\n        boolean success = false;\n        java.io.InputStream in = null;\n        java.io.OutputStream out = null;\n        try {\n            in = new Base64.InputStream(new java.io.BufferedInputStream(new java.io.FileInputStream(infile)), Base64.ENCODE);\n            out = new java.io.BufferedOutputStream(new java.io.FileOutputStream(outfile));\n            byte[] buffer = new byte[65536];\n            int read = -1;\n            while ((read = in.read(buffer)) >= 0) {\n                out.write(buffer, 0, read);\n            }\n            success = true;\n        } catch (java.io.IOException exc) {\n            exc.printStackTrace();\n        } finally {\n            try {\n                in.close();\n            } catch (Exception exc) {\n            }\n            try {\n                out.close();\n            } catch (Exception exc) {\n            }\n        }\n        return success;\n    }\n',
                'adversarial_code': '    public static boolean encodeFileToFile(String infile, String outfile) {\n        boolean success = false;\n        java.io.InputStream FTPClient = null;\n        java.io.OutputStream out = null;\n        try {\n            FTPClient = new Base64.InputStream(new java.io.BufferedInputStream(new java.io.FileInputStream(infile)), Base64.ENCODE);\n            out = new java.io.BufferedOutputStream(new java.io.FileOutputStream(outfile));\n            byte[] buffer = new byte[65536];\n            int read = -1;\n            while ((read = FTPClient.read(buffer)) >= 0) {\n                out.write(buffer, 0, read);\n            }\n            success = true;\n        } catch (java.io.IOException exc) {\n            exc.printStackTrace();\n        } finally {\n            try {\n                FTPClient.close();\n            } catch (Exception exc) {\n            }\n            try {\n                out.close();\n            } catch (Exception exc) {\n            }\n        }\n        return success;\n    }\n',
                'replaced_identifiers': {'in': 'FTPClient'},
                'query_times': 21,
                'time_cost': 0.024727197488149007,
                'error': None
            }
        finally:
            logger.info("\n" + "=" * 60)
            logger.info("âœ“ æ”»å‡»ä»»åŠ¡ç»“æŸ")
            logger.info("=" * 60)
    
 
